
# Introduction

Accelerometer devices objectively measure the wrist acceleration of human participants @Migueles2019. The derived features calculated from the accelerometer three axial time series aggregated per minute for each participant during a certain period of time can be used to estimate rest-activity rhythm, daytime activity, sleep, and chronotype. These features can be used to predict several health outcomes such as mortality @Chen2022, cardio-metabolic diseases @Cassidy2018, and neurodegenerative diseases @Nguyen2023. This represents a vast potential for medical research @Chan2022 as these devices provide a more objective and detailed information than commonly used self-reported data.

In survival analysis, we model the duration from a time origin until an event of interest occurs, considering a set of predictors. In case of several candidate predictors, some selection and/or combination of predictors are often operated to allow for a parsimonious model, using machine learning (ML) methods such as the sparse partial least squares deviance residuals (sPLSDR) model @Bastien2014.

ML methods often require tuning parameters to be fitted. These parameters are usually pre-estimated using cross-validation (CV) techniques. The general recommendation consists of doing 3 to 10 folds (depending on the sample size) @Lecao2021, within a CV with all combinations of hyper-parameters. The general rule is $n/k>5$, where $n$ is the sample size and $k$ the number of folds @Lecao2021(page 82). The sPLSDR requires two hyper-parameters: First $\eta \in [0,1)$, where 0 means a full dense model with all features, and values close to 1 a highly sparse model with only few features retained; second $D$ components (each component is a combination of features as in PLS regression). One approach consists of determining a large maximum number of components $D$ and a large grid for $\eta$, later trying a finer grid for $\eta$ only on the best options of $D$ @Chen2022. To evaluate the hotspots which are considering the best hyperparameter options must be based on a criterion, eg, integrated AUC survival ROC (iAUC) @Li2006. Even if this approach is commonly used, there are situations when it lacks enough stability.

It happens that when the outcome of interest is rare, for example major neurocognitive disorder (NCD), the process of optimisation is unstable for different fold values, and even for the same fold value, it may vary according the initial seed, indicating a lack of reliability. This instability could be due to small sample size where 5 folds for CV are recommended, but we hypothesise that the low proportion of cases by non-cases could be problematic even for large and medium sample size studies.

This paper will discuss ways to overcome the challenge of tuning parameter estimation in cases of rare events. Our application is based on the identification of accelerometer data predicting major NCD, such as Alzheimer's disease, using the Whitehall II ($n=3969$) as a training set and the UK Biobank (UKB) as a validation set. Our proposition to get stable results for the tuning parameters is to keep all the data from the training set and bootstrap it, and then to test the validity of the model using in an external dataset (UKB). The optimization process for Whitehall II is unstable for different fold values, and even for the same fold value (with different initial seed), which indicates a lack of reliability. The instability is not due to a small sample size, as Whitehall II is a relatively large cohort. However, the outcome presents a small proportion of incident events (1.4\%, the target disease is the major NCD, which includes Alzheimerâ€™s disease). The problem was not fixed when considering the rules for small sample size (5 folds) which is far from the bottom level $n/k>5$ suggest by @Lecao2021(page 82).

A small simulation study was conducted to test the hypotheses of low stability in case of small ratio between cases and non-cases. We artificially created data sets with $n=5000$ where the maximum observation time is 10 years of a disease for which incidence over time is small (resulting in few cases occurring during the observation period). In these simulations, we divided the data into five folds or keep the whole pool together, and we estimated the models using traditional performance criteria, such as the C-index and the Akaike information criterion (AIC).

# Method

## The sPLS technique

The sparse partial least squares regression (sPLS) was proposed by @Chun2010 to promote variable selection during PLS dimension reduction. The sPLS can select variables for the first direction vector and additional variables in the construction of the subsequent direction vectors @Bastien2014.

A direct extension of PLS regression to sPLS regression could be provided by imposing $L_1$ constraint on PLS direction on $n$-vector $w$ as $$
\max_{w} w^T M w \text{ subject to } w^Tw = ||w||_2 = 1, ||w||_1 \leq c,
$$ {#eq-spls} where $||w||_2 = w^Tw$, $||w||_1=\sum_{i=1}^{n} |w_i|$, $|u|$ is the absolute of $u$, $M=X^T Y Y^T X$, $Y$ and $X$ are matrices with the same number of rows, $c > 0$ is the $L_1$ constraint. If $Y=X$, then we have the special case sparse principal component analysis (sPCA) @Jolliffe2003.

## The Cox proportional hazards model

The model assumes the following hazard function for the occurrence of an event at time $t$ in the presence of censoring as follows $h(t) = h_0(t) \exp( \beta^T X),$ where $h_0(t)$ is a baseline hazard function, $\beta$ is a vector of regression coefficients and $X = (X_1,\dots,X_p)$ is a $n \times p$ matrix of features with $x_i = (x_{i1}, \dots, x_{ip})$ a row-vector for the i-th individual. The event could be any disease of interest. Based on the available data, Cox's partial likelihood can be written as $$
    L(\beta) = \prod_{k \in \Omega} \frac{\exp( \beta^T x_k)}{\sum_{j \in \zeta_k} \exp( \beta^T x_j)},
$$ where $\Omega$ is the set of indices of the events and $\zeta_k$ denotes the set of indices of the individuals at risk at time $t_k$. The goal is to find the coefficients which maximize the log partial likelihood function @Bastien2014.

## Deviance residuals

Here, we consider a Cox model with no time-dependent explanatory variables and, at most, one event per patient. Regardless, the Martingale residuals for the i-th subject with observation time $t_i$ and event status $\delta_i$, where $\delta_i=0$ if $t_i$ is a censored time, and $\delta_i=1$ otherwise is as $$
m_i = \delta_i - \hat{\Delta}_0(t_i) \exp (\hat{\beta}^T x_i),
$$ where $\hat{\Delta}_0(t_i)$ is the estimated cumulative hazard function at time $t_i$ @Bastien2014. However, the Martingale residuals used to be highly skewed @Bastien2014. Although, the deviance residuals $r_i$ are a normalized transform of the martingale residuals. For the Cox model, the deviance residuals @Collett1994 amount to a form as $$  
r_i = \mathrm{sign}  (m_i) [2\{ - r_i - \delta_i \log (\delta_i - m_i)\}]^{0.5},
$$ {#eq-residuals}

where $\mathrm{sign}(u)$ is the sign of $u \in \mathbb{R}$.

## The sPLSDR model

The sparse partial least squares deviance residual (sPLSDR) model was proposed by @Bastien2014 as an upgrade concerning sPLS proposed by @Chun2010 and partial least squares Cox (PLS-Cox) @Bastien2001, @Bastien2008. The general idea is to apply the sPLS technique presented in @eq-spls on the deviance residual from @eq-residuals.

The algorithm was described in @Bastien2014 as folllows

1.  Cox model without covariates to derive the null deviance residuals $r = (r_1, \dots, r_n)^T$ from @eq-residuals
2.  Computation of the sPLS components by using the sPLS regression from @eq-spls with the null deviance residuals as outcome.
3.  Set $\hat{\beta}^{PLS}=0$, $d=1$, $Y=r$,
4.  While $d \leq D$

```{=html}
<!-- -->
```
a.  $w = (|z|-c/2)+ \mathrm{sign}(z)$, where $z=X^TY/|| X^TY||_2$,
b.  Update $\Omega$ as $\{d : \hat{w}_d \neq 0\} \cup \{d : \hat{\beta}^{PLS}_d \neq 0\}$
c.  Fit sPLS with $X$ by using the $d$ number of latent components.
d.  Update $\hat{\beta}^{PLS}$ by using the new sPLS estimates of the direction vectors and update $Y$ and $d$ through $Y \leftarrow Y-X\hat{\beta}^{PLS}$ and $d \leftarrow d+1$

```{=html}
<!-- -->
```
5.  Cox model on the $D$ retained sPLSDR components.

# Monte Carlo study

## Simulation plan

We used an exponential survival model, where the hazard function is 
$$
h_i(t) = \lambda \exp( X_i \beta ),
$${#eq-sim} 
and the cumulative hazard is $$H_i(t) = \lambda t \exp( X_i \beta ),$$ such as implemented on the **simsurv** R Package by @Brilleman2021. We create artificial data with $n=5000$ number of individuals, $\lambda=0.05, 0.005$ rate parameter, $\beta^T = (\beta^T_1, \beta^T_2, \beta^T_3)$ is the coefficient parameters, composed by $\beta^T_1 = (0.15, 0.1, 0.05)$, $\beta^T_2 = (-0.15, -0.1, -0.05)$ and\
$\beta^T_3=(0,0,0,0,0,0)$; the covariate matrix is $X_{i,(1:3)} \sim N(\mu_1, R)$ a multivariate normal with $\mu_1=(0.15, 0.1, 0.05)^T$ and $R$ a matrix with variance on the diagonal of ones and covariance on the off-diagonals of 0.5; $X_{i,(4:6)} \sim N(\mu_2, R)$ a multivariate normal with $\mu_2=(-0.15, -0.1, -0.05)^T$ and same covariance; $X_{i,(7:12)} \sim N(\mu_3, I)$ a multivariate normal with $\mu_3=(0,0,0,0,0,0)^T$ and $I$ an identity matrix. The maximum observed time is $m=10$.

So this simple model may be well estimated by a sPLSDR model with two dimensions. We proceed with a Monte Carlo simulation, replicating $M=1000$ times each $\lambda$ and estimating the model changing $\eta=0, 0.1, 0.2, \dots, 0.9$, using $k=1,5$ folds. We save the C-index and the AIC of each iteration of the Monte Carlo. We set all models with two dimensions ($D=2$). Finally, we count how many times each value of $\eta$ was considered the best in the row; this results in a frequency per value of $\eta$.

## Simulation implementation

Before the results, we implemented some functions available in the R folder. Also, some packages are necessary, such as **plsRcox** (the original package to estimate sPLSDR @Bastien2014), **Coxmos** (another package to estimate sPLSDR but with a convenient way to extract the $W^*$ matrix of weights, this matrix is similar to a loading matrix in PCA @Coxmos), **caret** (to split the dataset in folds @caret).


```{r  message=FALSE, warning=FALSE}
# Set repositories
options(
  repos = c(CRAN = "https://cloud.r-project.org"),
  BioC_mirror = "https://bioconductor.org"
)

# Helper: install a package only if missing
install_if_missing <- function(pkg, bioc = FALSE) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    if (bioc) {
      if (!requireNamespace("BiocManager", quietly = TRUE))
        install.packages("BiocManager", repos = "https://cloud.r-project.org")
      BiocManager::install(pkg, ask = FALSE)
    } else {
      install.packages(pkg, repos = "https://cloud.r-project.org")
    }
  }
}

# Install required packages (only if missing)
install_if_missing("mixOmics", bioc = TRUE)
install_if_missing("survcomp", bioc = TRUE)
#install_if_missing("plsRcox")
install_if_missing("Coxmos")
install_if_missing("simsurv")
install_if_missing("caret")
install_if_missing("MASS")

# Load libraries
library(survcomp)
#library(mixOmics)
#library(plsRcox)
library(Coxmos)
library(simsurv)
library(caret)
library(MASS)

# Source functions from GitHub
# source("https://raw.githubusercontent.com/iandanilevicz/sPLSDRtuning/main/R/functions3.R")
source("R/functions3.R")
```


